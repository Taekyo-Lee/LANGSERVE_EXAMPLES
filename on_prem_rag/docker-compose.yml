services:
  web:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./app:/code/app  # Volume 1: Mount your local app directory to /code/app in the container
      - ./packages/opensource_rag:/code/packages/opensource_rag  # Volume 2: Mount the package directory to the container
    depends_on:
      - vllm
      - qdrant
      - ollama
    env_file:
      - ./env.list
    command: ["uvicorn", "app.server:app", "--host", "0.0.0.0", "--port", "8080", "--reload"]

  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia  # Use the NVIDIA runtime
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    env_file:
      - ./env.list
    volumes:
      - hf_data:/root/.cache/huggingface
    ports:
      - "8888:8000"  # Exposing VLLM's internal port 8888 (Inside of the container, use 8000)
    ipc: "host"  # Use host IPC namespace for shared memory
    command: ["--model", "deepseek-ai/deepseek-llm-7b-chat", "--max_model_len", "640"]

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  qdrant_data:
  ollama_data:
  hf_data:
